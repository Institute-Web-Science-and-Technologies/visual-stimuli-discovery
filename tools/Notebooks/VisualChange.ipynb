{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "\n",
    "This script takes the visual change dataset and provides the f1-scores for different classifiers estimating visual change on various subsets of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import os.path\n",
    "from collections import defaultdict\n",
    "\n",
    "# Custom modules\n",
    "from classifier import Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "min_obs_extent = 32 # extent too which obs are withdrawn \n",
    "baseline_feature_name = 'pixel_diff_count_bgr' # 'n_grams_jaccard' (must be not dropped by features_drop or features_filter)\n",
    "\n",
    "# Drop and filter features\n",
    "features_drop = [\n",
    "    'bag_of_words_vocabulary_size',\n",
    "    'optical_flow_angle_min',\n",
    "    'optical_flow_angle_max',\n",
    "    'optical_flow_magnitude_min']\n",
    "features_filter = [ # not applied if empty (all features but dropped ones are then considered)\n",
    "    #'edge_change_fraction',\n",
    "    #'mssim_b',\n",
    "    #'mssim_g',\n",
    "    #'mssim_r',\n",
    "    #'pixel_diff_acc_b',\n",
    "    #'pixel_diff_acc_bgr',\n",
    "    #'pixel_diff_acc_g',\n",
    "    #'pixel_diff_acc_gray',\n",
    "    #'pixel_diff_acc_hue',\n",
    "    #'pixel_diff_acc_lightness',\n",
    "    #'pixel_diff_acc_r',\n",
    "    #'pixel_diff_acc_saturation',\n",
    "    #'pixel_diff_count_b',\n",
    "    #'pixel_diff_count_bgr',\n",
    "    #'pixel_diff_count_g',\n",
    "    #'pixel_diff_count_gray',\n",
    "    #'pixel_diff_count_hue',\n",
    "    #'pixel_diff_count_lightness',\n",
    "    #'pixel_diff_count_r',\n",
    "    #'pixel_diff_count_saturation',\n",
    "    #'psnr',\n",
    "    #'sift_match',\n",
    "    #'sift_match_0',\n",
    "    #'sift_match_16',\n",
    "    #'sift_match_256',\n",
    "    #'sift_match_4',\n",
    "    #'sift_match_512',\n",
    "    #'sift_match_64',\n",
    "    #'sift_match_distance_max',\n",
    "    #'sift_match_distance_mean',\n",
    "    #'sift_match_distance_min',\n",
    "    #'sift_match_distance_stddev',\n",
    "    #'sift_match_spatial'\n",
    "]\n",
    "\n",
    "# Defines\n",
    "dataset_visual_change_dir = r'C:/StimuliDiscoveryData/Dataset_visual_change' # SET ME!\n",
    "participants = ['p1', 'p2', 'p3', 'p4']\n",
    "\n",
    "# Categories\n",
    "shopping = ['walmart', 'amazon', 'steam']\n",
    "news = ['reddit', 'cnn', 'guardian']\n",
    "health = ['nih', 'webmd', 'mayo']\n",
    "cars = ['gm', 'nissan', 'kia']\n",
    "categories = {'shopping': shopping, 'news': news, 'health': health, 'cars': cars}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Session class holds one site visit by a participant\n",
    "class Session:\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self, participant, site):\n",
    "        \n",
    "        # Store some members of general interest\n",
    "        self.p = participant\n",
    "        self.s = site\n",
    "        \n",
    "        # Load dataset (header of features dataset has extra comma)\n",
    "        self.f_df = pd.read_csv(dataset_visual_change_dir + '/' + participant + '/' + site + '_features.csv')\n",
    "        self.mf_df = pd.read_csv(dataset_visual_change_dir + '/' + participant + '/' + site + '_features_meta.csv')\n",
    "        self.l1_df = pd.read_csv(dataset_visual_change_dir + '/' + participant + '/' + site + '_labels-l1.csv', header=None, names=['label'])\n",
    "        self.m_df = pd.read_csv(dataset_visual_change_dir + '/' + participant + '/' + site + '_meta.csv')\n",
    "        \n",
    "        # Load additional labeling by external person if available\n",
    "        file_path = dataset_visual_change_dir + '/' + participant + '/' + site + '_labels-l2.csv'\n",
    "        self.l2_df = pd.DataFrame\n",
    "        if os.path.exists(file_path):\n",
    "            self.l2_df = pd.read_csv(file_path, header=None, names=['label'])\n",
    "        \n",
    "        # Drop columns of non-interest\n",
    "        self.f_df = self.f_df.drop(features_drop, axis=1)\n",
    "        \n",
    "        # Filter for columns of interest\n",
    "        if len(features_filter) > 0:\n",
    "            self.f_df = self.f_df.filter(items=features_filter, axis=1)\n",
    "        \n",
    "        # Drop observations that are smaller than a certain extent\n",
    "        width_idxs = self.mf_df[self.mf_df['overlap_width'] <= min_obs_extent].index\n",
    "        height_idxs = self.mf_df[self.mf_df['overlap_height'] <= min_obs_extent].index\n",
    "        drop_idxs = list(set(width_idxs) | set(height_idxs))\n",
    "        self.f_df = self.f_df.drop(drop_idxs, axis=0)\n",
    "        self.mf_df = self.mf_df.drop(drop_idxs, axis=0)\n",
    "        self.l1_df = self.l1_df.drop(drop_idxs, axis=0)\n",
    "        if not self.l2_df.empty: self.l2_df = self.l2_df.drop(drop_idxs, axis=0)\n",
    "        self.skipped = len(drop_idxs)\n",
    "        \n",
    "    # Compare labelings\n",
    "    def compare_labeling(self):\n",
    "        if not self.l2_df.empty:            \n",
    "            report = classification_report(self.l1_df.values, self.l2_df.values, output_dict=True)\n",
    "            print('-> L1 VS L2 for ' + self.p + ', ' + self.s)\n",
    "            print('Cohen Kappa Score: ' + str(cohen_kappa_score(self.l1_df.values, self.l2_df.values))) # 70% is good\n",
    "            \n",
    "            # Classes\n",
    "            class_0 = report['0.0']\n",
    "            class_1 = report['1.0']\n",
    "\n",
    "            # Values\n",
    "            print('Class 0: ', end='')\n",
    "            print('Precision: ' + f\"{class_0['precision']:.2f}\" + ', ', end='') \n",
    "            print('Recall: ' + f\"{class_0['recall']:.2f}\" + ', ', end='')\n",
    "            print('F1-Score: ' + f\"{class_0['f1-score']:.2f}\")\n",
    "            print('Class 1: ', end='')\n",
    "            print('Precision: ' + f\"{class_1['precision']:.2f}\" + ', ', end='') \n",
    "            print('Recall: ' + f\"{class_1['recall']:.2f}\" + ', ', end='')\n",
    "            print('F1-Score: ' + f\"{class_1['f1-score']:.2f}\")\n",
    "            \n",
    "            # Return information about success\n",
    "            return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import sessions of each category\n",
    "sessions = []\n",
    "for category, sites in categories.items():\n",
    "    for site in sites:\n",
    "        for p in participants:\n",
    "            sessions.append(Session(p, site))\n",
    "            \n",
    "# Count observations and labels with value of one\n",
    "labeled_obs_count = 0\n",
    "skipped_pixel_perfect_obs_count = 0\n",
    "skipped_overlap_extent_count = 0\n",
    "total_obs_count = 0\n",
    "diff_count = 0\n",
    "screencast_seconds = 0.0\n",
    "frame_count = 0\n",
    "for session in sessions:\n",
    "    labeled_obs_count += len(session.f_df.index) # aggregate observations (without skipped, those for which are features computed)\n",
    "    skipped_pixel_perfect_obs_count += session.m_df['observation_count_skipped'].values[0] # aggregate skipped observations\n",
    "    skipped_overlap_extent_count += session.skipped # skipped because of overlap extent\n",
    "    total_obs_count += session.m_df['observation_total_count'].values[0] # aggregate total observations\n",
    "    diff_count += np.count_nonzero(session.l1_df.values) # aggregate nonzero labels\n",
    "    screencast_seconds += session.m_df['screencast_seconds'].values[0] # aggregate screencast duration\n",
    "    frame_count += session.m_df['screencast_frame_total_count'].values[0] # aggregate frame count from screen cast\n",
    "    \n",
    "print('Screencast seconds: ' + str(screencast_seconds))\n",
    "print('Screencast minutes: ' + str(screencast_seconds/60))\n",
    "print('Count of labeled observations: ' + str(labeled_obs_count))\n",
    "print('Count of skipped observations (because pixel-perfect similar): ' + str(skipped_pixel_perfect_obs_count))\n",
    "print('Count of skipped observations (because overlap too small): ' + str(skipped_overlap_extent_count))\n",
    "print('Count of total observations: ' + str(total_obs_count))\n",
    "print('Count of observations labeled as visual different: ' + str(diff_count))\n",
    "print('Count of frames in screen casts: ' + str(frame_count))\n",
    "print()\n",
    "print('### Compare different labelings')\n",
    "print()\n",
    "for session in sessions:\n",
    "    if session.compare_labeling():\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features Computation Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import sessions of each category\n",
    "sessions = []\n",
    "for category, sites in categories.items():\n",
    "    for site in sites:\n",
    "        for p in participants:\n",
    "            sessions.append(Session(p, site))\n",
    "\n",
    "# Collect all entries\n",
    "sift_match = []\n",
    "pixel_diff = []\n",
    "optical_flow = []\n",
    "ocr = []\n",
    "histogram = []\n",
    "edge = []\n",
    "psnr = []\n",
    "mssim = []\n",
    "n_grams = []\n",
    "for session in sessions:\n",
    "    sift_match.append(session.mf_df['sift_match_features [ms]'].values)\n",
    "    pixel_diff.append(session.mf_df['pixel_diff_features [ms]'].values)\n",
    "    optical_flow.append(session.mf_df['optical_flow_features [ms]'].values)\n",
    "    ocr.append(session.mf_df['ocr_descriptors [ms]'].values)\n",
    "    histogram.append(session.mf_df['histogram_descriptors [ms]'].values)\n",
    "    edge.append(session.mf_df['edge_change_ratio_features [ms]'].values)\n",
    "    psnr.append(session.mf_df['psnr_features [ms]'].values)\n",
    "    mssim.append(session.mf_df['mssim_features [ms]'].values)\n",
    "    n_grams.append(session.mf_df['n_grams_features [ms]'].values)\n",
    "\n",
    "# Flatten the collected entries\n",
    "sift_match = np.concatenate(sift_match).ravel()\n",
    "pixel_diff = np.concatenate(pixel_diff).ravel()\n",
    "optical_flow = np.concatenate(optical_flow).ravel()\n",
    "ocr = np.concatenate(ocr).ravel()\n",
    "histogram = np.concatenate(histogram).ravel()\n",
    "edge = np.concatenate(edge).ravel()\n",
    "psnr = np.concatenate(psnr).ravel()\n",
    "mssim = np.concatenate(mssim).ravel()\n",
    "n_grams = np.concatenate(n_grams).ravel()\n",
    "\n",
    "# Print information\n",
    "print('### Feature Computation Timings in Milliseconds')\n",
    "print('Sift Match:   ' + f'{np.mean(sift_match):.2f}' + '±' + f'{np.std(sift_match):.2f}')\n",
    "print('Pixel Diff:   ' + f'{np.mean(pixel_diff):.2f}' + '±' + f'{np.std(pixel_diff):.2f}')\n",
    "print('Optical Flow: ' + f'{np.mean(optical_flow):.2f}' + '±' + f'{np.std(optical_flow):.2f}')\n",
    "print('OCR:          ' + f'{np.mean(ocr):.2f}' + '±' + f'{np.std(ocr):.2f}')\n",
    "print('Histogram:    ' + f'{np.mean(histogram):.2f}' + '±' + f'{np.std(histogram):.2f}')\n",
    "print('Edge:         ' + f'{np.mean(edge):.2f}' + '±' + f'{np.std(edge):.2f}')\n",
    "print('PSNR:         ' + f'{np.mean(psnr):.2f}' + '±' + f'{np.std(psnr):.2f}')\n",
    "print('MSSIM:        ' + f'{np.mean(mssim):.2f}' + '±' + f'{np.std(mssim):.2f}')\n",
    "print('N-Grams:      ' + f'{np.mean(n_grams):.2f}' + '±' + f'{np.std(n_grams):.2f}')\n",
    "\n",
    "sum = np.mean(sift_match) + np.mean(pixel_diff) + np.mean(optical_flow) + np.mean(ocr) + np.mean(histogram) + np.mean(edge) + np.mean(psnr) + np.mean(mssim) + np.mean(n_grams)\n",
    "print('Sum: '+ f'{sum:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base class of learners\n",
    "class Learner:\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self):\n",
    "        self.importances = defaultdict(list) # this is a dictionary with feature_name as key and list of importance values as value\n",
    "        self.avg_class0_f1 = 0.0\n",
    "        self.avg_class1_f1 = 0.0\n",
    "        self.visual_changes_rf = [] # visual changes as computed by random forest classifier\n",
    "    \n",
    "    # Compute learning on sessions. Returns dictionary with predictions from different classifiers\n",
    "    def compute(self, sessions, idxs_test):\n",
    "        \n",
    "        # Compose training set\n",
    "        features_train = []\n",
    "        labels_train = []\n",
    "        idxs_train = set(range(len(sessions))) - set(idxs_test)\n",
    "        for idx in idxs_train:\n",
    "            features_train.append(sessions[idx].f_df)\n",
    "            labels_train.append(sessions[idx].l1_df)\n",
    "        f_train_df = pd.concat(features_train, ignore_index=True)\n",
    "        l1_train_df = pd.concat(labels_train, ignore_index=True)\n",
    "        \n",
    "        # Compose test set\n",
    "        features_test = []\n",
    "        labels_test = []\n",
    "        for idx in idxs_test:\n",
    "            features_test.append(sessions[idx].f_df)\n",
    "            labels_test.append(sessions[idx].l1_df)\n",
    "        f_test_df = pd.concat(features_test, ignore_index=True)\n",
    "        l1_test_df = pd.concat(labels_test, ignore_index=True)\n",
    "        \n",
    "        # Replace some values (but use complete training set for estimation)\n",
    "        if 'optical_flow_magnitude_max' in f_train_df.columns:\n",
    "            \n",
    "            # Replace infinity datapoints in 'optical_flow_magnitude_max' with maximum value (encoded as -1)\n",
    "            max_value = f_train_df['optical_flow_magnitude_max'].max() # maximum from complete training data\n",
    "\n",
    "            # In both, training an test data\n",
    "            f_train_df[f_train_df['optical_flow_magnitude_max'] == -1] = max_value\n",
    "            f_test_df[f_test_df['optical_flow_magnitude_max'] == -1] = max_value\n",
    "        \n",
    "        # Convert pandas dataframe to numpy array\n",
    "        X_train = f_train_df.values\n",
    "        y_train = l1_train_df.values.flatten()\n",
    "        X_test = f_test_df.values\n",
    "        y_test = l1_test_df.values.flatten()\n",
    "        \n",
    "        # Use machine to predict labels of test data\n",
    "        classifier = Classifier()\n",
    "        idx_baseline = f_train_df.columns.get_loc(baseline_feature_name)\n",
    "        pred = classifier.apply(X_train, y_train, X_test, idx_baseline)\n",
    "        pred['truth'] = y_test # adding ground truth to the prediction\n",
    "        \n",
    "        # Also apply the classifier on the entire input data as test data (which includes training data)\n",
    "        # This allows us to estimate the number of computed shot boundaries across the entire input data and compare\n",
    "        # to related work in scene segmentation\n",
    "        features_complete = []\n",
    "        for session in sessions:\n",
    "            features_complete.append(session.f_df)\n",
    "        f_complete_df = pd.concat(features_complete, ignore_index=True)\n",
    "        pred_complete = classifier.apply(X_train, y_train, f_complete_df.values, idx_baseline)\n",
    "        self.visual_changes_rf.append(np.count_nonzero(pred_complete['forest']))\n",
    "        \n",
    "        # Store predictions\n",
    "        # TODO: split computed labels by test sessions\n",
    "        # TODO: provide kind of test as method parameter (e.g., same-site)\n",
    "        # pd.DataFrame(pred['forest']).to_csv(dataset_dir + participant + '/' + site + '_l_forest.csv')\n",
    "        \n",
    "        # Return predictions\n",
    "        return pred\n",
    "    \n",
    "    # Compute some members\n",
    "    def analyze_predictions(self, preds, feature_names):\n",
    "        \n",
    "        # Go over predictions\n",
    "        for pred in preds:\n",
    "            \n",
    "            # Store feature importance\n",
    "            for i in range(len(pred['importance'])):\n",
    "                self.importances[feature_names[i]].append(pred['importance'][i])\n",
    "                \n",
    "            # Store f1-score\n",
    "            score = f1_score(pred['truth'], pred['forest'], average=None) # f1-scores for class 0 and class 1\n",
    "            self.avg_class0_f1 += score[0]\n",
    "            self.avg_class1_f1 += score[1]\n",
    "            \n",
    "        # Average the scores\n",
    "        self.avg_class0_f1 /= len(preds)\n",
    "        self.avg_class1_f1 /= len(preds)\n",
    "    \n",
    "    # Report about learning\n",
    "    def report(self, preds, feature_names):\n",
    "    \n",
    "        # Report metrics by collecting all reports across idxs_test, per classifier\n",
    "        reports = {'logreg': [], 'svc': [], 'forest': [], 'baseline': []}\n",
    "        importances = []\n",
    "        for pred in preds:\n",
    "            \n",
    "            # Generate classification reports from classifier predictions\n",
    "            reports['logreg'].append(classification_report(pred['truth'], pred['logreg'], output_dict=True))\n",
    "            reports['svc'].append(classification_report(pred['truth'], pred['svc'], output_dict=True))\n",
    "            reports['forest'].append(classification_report(pred['truth'], pred['forest'], output_dict=True))\n",
    "            reports['baseline'].append(classification_report(pred['truth'], pred['baseline'], output_dict=True))\n",
    "            \n",
    "            # Store feature importance\n",
    "            for i in range(len(pred['importance'])):\n",
    "                if i >= len(importances):\n",
    "                    importances.append([])\n",
    "                importances[i].append(pred['importance'][i])\n",
    "                \n",
    "        # Comprehend importances and sort list of feature names accordingly\n",
    "        importances = [np.mean(x) for x in importances]\n",
    "        important_features = sorted(zip(importances, feature_names), reverse=True) # the higher, the more important\n",
    "        \n",
    "        # Collect all metrics in a nested dictionary classifier -> metrics\n",
    "        self.classifier_metrics = {}\n",
    "        \n",
    "        # Go over classifiers\n",
    "        for classifier, reports in reports.items():\n",
    "            \n",
    "            # Compose report\n",
    "            precision_0 = []\n",
    "            precision_1 = []\n",
    "            recall_0 = []\n",
    "            recall_1 = []\n",
    "            f1_score_0 = []\n",
    "            f1_score_1 = []\n",
    "            support_0 = [] # not yet used\n",
    "            support_1 = [] # not yet used\n",
    "\n",
    "            # Go over reports for specific classifier\n",
    "            for report in reports:\n",
    "\n",
    "                # Classes\n",
    "                class_0 = report['0.0']\n",
    "                class_1 = report['1.0']\n",
    "\n",
    "                # Values\n",
    "                precision_0.append(class_0['precision'])\n",
    "                recall_0.append(class_0['recall'])\n",
    "                f1_score_0.append(class_0['f1-score'])\n",
    "                support_0.append(class_0['support'])\n",
    "                precision_1.append(class_1['precision'])\n",
    "                recall_1.append(class_1['recall'])\n",
    "                f1_score_1.append(class_1['f1-score'])\n",
    "                support_1.append(class_1['support'])\n",
    "                \n",
    "            # Fill metrics into dictionary for the classifier\n",
    "            metrics = {}\n",
    "            metrics['precision_0'] = [np.mean(precision_0), np.std(precision_0)]\n",
    "            metrics['recall_0']    = [np.mean(recall_0),    np.std(recall_0)]\n",
    "            metrics['f1-score_0']  = [np.mean(f1_score_0),  np.std(f1_score_0)]\n",
    "            metrics['precision_1'] = [np.mean(precision_1), np.std(precision_1)]\n",
    "            metrics['recall_1']    = [np.mean(recall_1),    np.std(recall_1)]\n",
    "            metrics['f1-score_1']  = [np.mean(f1_score_1),  np.std(f1_score_1)]\n",
    "            self.classifier_metrics[classifier] = metrics\n",
    "\n",
    "        # Print report of mean and stddev of metrics\n",
    "        metric_strings = ['precision_0', 'recall_0', 'f1-score_0', 'precision_1', 'recall_1', 'f1-score_1']\n",
    "        \n",
    "        # Print header\n",
    "        print(''.ljust(14), end='')\n",
    "        for classifier in self.classifier_metrics.keys():\n",
    "            print(classifier.ljust(11), end='')\n",
    "        print('Ranked Important Features (by mean importance)')\n",
    "        \n",
    "        # Print values\n",
    "        i = 0\n",
    "        for metric_string in metric_strings:\n",
    "            print(metric_string.rjust(12) + '  ', end='')\n",
    "            for classifier, metrics in self.classifier_metrics.items():\n",
    "                metric = metrics[metric_string]\n",
    "                print(f'{metric[0]:.2f}' + '±' + f'{metric[1]:.2f}', end='  ')\n",
    "            if i < len(important_features):\n",
    "                print(important_features[i][1].ljust(32) + '' + str(important_features[i][0]), end='')\n",
    "            print()\n",
    "            i += 1\n",
    "            \n",
    "        # Print some more important features\n",
    "        while i < 15 and i < len(important_features):\n",
    "            print('                                                          ', end='')\n",
    "            print(important_features[i][1].ljust(32) + '' + str(important_features[i][0]), end='')\n",
    "            print()\n",
    "            i += 1\n",
    "            \n",
    "    # Report about learning for paper (latexish code)\n",
    "    def report_paper(self, preds, feature_names):\n",
    "    \n",
    "        # Report metrics by collecting all reports across idxs_test, per classifier\n",
    "        reports = {'svc': [], 'forest': [], 'baseline': []}\n",
    "        importances = []\n",
    "        for pred in preds:\n",
    "            \n",
    "            # Generate classification reports from classifier predictions\n",
    "            reports['svc'].append(classification_report(pred['truth'], pred['svc'], output_dict=True))\n",
    "            reports['forest'].append(classification_report(pred['truth'], pred['forest'], output_dict=True))\n",
    "            reports['baseline'].append(classification_report(pred['truth'], pred['baseline'], output_dict=True))\n",
    "        \n",
    "        # Collect all metrics in a nested dictionary classifier -> metrics\n",
    "        self.classifier_metrics = {}\n",
    "        \n",
    "        # Go over classifiers\n",
    "        for classifier, reports in reports.items():\n",
    "            \n",
    "            # Compose report\n",
    "            f1_score_0 = []\n",
    "            f1_score_1 = []\n",
    "            \n",
    "            # Go over reports for specific classifier\n",
    "            for report in reports:\n",
    "\n",
    "                # Classes\n",
    "                class_0 = report['0.0']\n",
    "                class_1 = report['1.0']\n",
    "\n",
    "                # Values\n",
    "                f1_score_0.append(class_0['f1-score'])\n",
    "                f1_score_1.append(class_1['f1-score'])\n",
    "                \n",
    "            # Fill metrics into dictionary for the classifier\n",
    "            metrics = {}\n",
    "            metrics['f1-score_0']  = [np.mean(f1_score_0),  np.std(f1_score_0)]\n",
    "            metrics['f1-score_1']  = [np.mean(f1_score_1),  np.std(f1_score_1)]\n",
    "            self.classifier_metrics[classifier] = metrics\n",
    "            \n",
    "        # Print report of mean and stddev of metrics\n",
    "        metric_strings = ['f1-score_0', 'f1-score_1']\n",
    "        \n",
    "        # Print header\n",
    "        print(''.ljust(14), end='')\n",
    "        for classifier in self.classifier_metrics.keys():\n",
    "            print(classifier.ljust(11), end='')\n",
    "        print()\n",
    "        \n",
    "        # Print values\n",
    "        for metric_string in metric_strings:\n",
    "            print(metric_string.rjust(12) + '  ', end='')\n",
    "            for classifier, metrics in self.classifier_metrics.items():\n",
    "                metric = metrics[metric_string]\n",
    "                print('$' + str(int(round(100*metric[0]))).zfill(2)  + '\\\\pm' + str(int(round(100*metric[1]))).zfill(2), end='$ & ')\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specific Learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Session Learner\n",
    "class OneSessionLearner(Learner):\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self, site):\n",
    "        \n",
    "        # Super\n",
    "        Learner.__init__(self)\n",
    "        \n",
    "        # Import sessions\n",
    "        self.sessions = []\n",
    "        for p in participants:\n",
    "            self.sessions.append(Session(p, site))\n",
    "            \n",
    "        # Perform learning for each session by using one session for training and three for test\n",
    "        self.preds = [] # list of predictions\n",
    "        for i in range(len(self.sessions)):\n",
    "            idxs_test = [x for x in range(len(self.sessions))] # first, all indices of sessions\n",
    "            idxs_test = list(filter(lambda x: x != i, idxs_test)) # then, remove the one training index\n",
    "            self.preds.append(self.compute(self.sessions, idxs_test))\n",
    "            \n",
    "        # Analyze predictions\n",
    "        self.analyze_predictions(self.preds, list(self.sessions[0].f_df.columns.values))\n",
    "            \n",
    "    # Print report\n",
    "    def print_report(self):\n",
    "    \n",
    "        # Report about predictions\n",
    "        self.report(self.preds, list(self.sessions[0].f_df.columns.values))\n",
    "        # self.report_paper(self.preds, list(self.sessions[0].f_df.columns.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same-Site Learner\n",
    "class SameSiteLearner(Learner):\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self, site):\n",
    "        \n",
    "        # Super\n",
    "        Learner.__init__(self)\n",
    "        \n",
    "        # Import sessions\n",
    "        self.sessions = []\n",
    "        for p in participants:\n",
    "            self.sessions.append(Session(p, site))\n",
    "            \n",
    "        # Perform learning for each session left out once\n",
    "        self.preds = [] # list of predictions\n",
    "        for i in range(len(self.sessions)):\n",
    "            self.preds.append(self.compute(self.sessions, [i]))\n",
    "        \n",
    "        # Analyze predictions\n",
    "        self.analyze_predictions(self.preds, list(self.sessions[0].f_df.columns.values))\n",
    "            \n",
    "    # Print report\n",
    "    def print_report(self):\n",
    "    \n",
    "        # Report about predictions\n",
    "        self.report(self.preds, list(self.sessions[0].f_df.columns.values))\n",
    "        # self.report_paper(self.preds, list(self.sessions[0].f_df.columns.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same-Category Learner\n",
    "class SameCategoryLearner(Learner):\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self, sites):\n",
    "        \n",
    "        # Super\n",
    "        Learner.__init__(self)\n",
    " \n",
    "        # Import sessions on each site\n",
    "        self.sessions = []\n",
    "        for site in sites:\n",
    "            for p in participants:\n",
    "                self.sessions.append(Session(p, site))\n",
    "                \n",
    "        # Perform learning for each site left out once\n",
    "        self.preds = [] # list of predictions\n",
    "        sessions_per_site = len(participants)\n",
    "        for i in range(len(sites)):\n",
    "            self.preds.append(self.compute(self.sessions, range(i*sessions_per_site, (i+1)*sessions_per_site)))\n",
    "        \n",
    "        # Analyze predictions\n",
    "        self.analyze_predictions(self.preds, list(self.sessions[0].f_df.columns.values))\n",
    "        \n",
    "    # Print report\n",
    "    def print_report(self):\n",
    "    \n",
    "        # Report about predictions\n",
    "        self.report(self.preds, list(self.sessions[0].f_df.columns.values))\n",
    "        # self.report_paper(self.preds, list(self.sessions[0].f_df.columns.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Across-Category Leaener\n",
    "class AcrossCategoryLearner(Learner):\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self, categories, test_category_name):\n",
    "        \n",
    "        # Super\n",
    "        Learner.__init__(self)\n",
    " \n",
    "        # Import sessions of each category\n",
    "        self.sessions = []\n",
    "        idxs_test = []\n",
    "        for category, sites in categories.items():\n",
    "            for site in sites:\n",
    "                for p in participants:\n",
    "                    self.sessions.append(Session(p, site))\n",
    "                    if test_category_name == category:\n",
    "                        idxs_test.append(len(self.sessions)-1)\n",
    "\n",
    "        # Perform learning (no stddev to expect...)\n",
    "        self.preds = [] # list of predictions\n",
    "        self.preds.append(self.compute(self.sessions, idxs_test))\n",
    "        \n",
    "        # Analyze predictions\n",
    "        self.analyze_predictions(self.preds, list(self.sessions[0].f_df.columns.values))\n",
    "        \n",
    "    # Print report\n",
    "    def print_report(self):\n",
    "    \n",
    "        # Report about predictions\n",
    "        self.report(self.preds, list(self.sessions[0].f_df.columns.values))\n",
    "        # self.report_paper(self.preds, list(self.sessions[0].f_df.columns.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_ranked_importances(raw_importances):\n",
    "    # https://stackoverflow.com/questions/15810339/how-are-feature-importances-in-randomforestclassifier-determined\n",
    "    importances_acc = [] # triples of mean, stddev, feature name\n",
    "    for feature_name, importances in raw_importances.items():\n",
    "        importances_acc.append([np.mean(importances), np.std(importances), feature_name])\n",
    "    importances_acc = sorted(importances_acc, reverse=True) # the higher, the more important\n",
    "    for [mean, std, feature_name] in importances_acc:\n",
    "        print(feature_name.ljust(32) + ': ' + f'{(100*mean):.2f}' + '±' + f'{(100*std):.2f}') # contribution in percentage\n",
    "    \n",
    "# One-Session learner\n",
    "print('One-Session-Learning (4-fold cross valiation, one session as training, three sessions as test)')\n",
    "avg_class0_f1 = 0.0\n",
    "avg_class1_f1 = 0.0\n",
    "i = 0\n",
    "one_session_importances = defaultdict(list)\n",
    "one_session_forest_precisions_1 = []\n",
    "one_session_forest_recalls_1 = []\n",
    "one_session_avg_visual_changes = []\n",
    "for name, sites in categories.items():\n",
    "    print()\n",
    "    print('#------------------- ' + name.center(14) + ' -------------------#')\n",
    "    print()\n",
    "    for site in sites:\n",
    "        print('                  <<<' + site.center(14) + '>>>                  ')\n",
    "        \n",
    "        # Learn\n",
    "        one_session_learner = OneSessionLearner(site)\n",
    "        \n",
    "        # Print detailed report\n",
    "        one_session_learner.print_report()\n",
    "        \n",
    "        # Collect count of visual changes\n",
    "        one_session_avg_visual_changes.append(np.mean(one_session_learner.visual_changes_rf)) # there is one for each fold, thus, take avg across folds\n",
    "        \n",
    "        # Collect importances\n",
    "        for feature_name, importances in one_session_learner.importances.items():\n",
    "            one_session_importances[feature_name] += importances\n",
    "        \n",
    "        # Collect precision and recall\n",
    "        one_session_forest_precisions_1.append(one_session_learner.classifier_metrics['forest']['precision_1'][0]) # zero for mean instead of std\n",
    "        one_session_forest_recalls_1.append(one_session_learner.classifier_metrics['forest']['recall_1'][0])\n",
    "        \n",
    "        # Collect f1-scores\n",
    "        avg_class0_f1 += one_session_learner.avg_class0_f1\n",
    "        avg_class1_f1 += one_session_learner.avg_class1_f1\n",
    "        i += 1\n",
    "        print()\n",
    "        \n",
    "# Normalize f1-scores and report\n",
    "avg_class0_f1 /= i\n",
    "avg_class1_f1 /= i\n",
    "print('Averaged F1-Score of class 0: ' + str(avg_class0_f1))\n",
    "print('Averaged F1-Score of class 1: ' + str(avg_class1_f1))\n",
    "print()\n",
    "        \n",
    "print('--- Ranked Feature Importances (already in percentage) ---')\n",
    "print_ranked_importances(one_session_importances)\n",
    "\n",
    "# Precision and recall for visual change\n",
    "print('--- Overall Precision and Recall (already in percentage) ---')\n",
    "one_session_forest_precisions_1_mean = np.mean(one_session_forest_precisions_1)\n",
    "one_session_forest_precisions_1_std = np.std(one_session_forest_precisions_1)\n",
    "print('Overall precision: '\n",
    "      + f'{(100*one_session_forest_precisions_1_mean):.2f}'\n",
    "      + '±'\n",
    "      + f'{(100*one_session_forest_precisions_1_std):.2f}')\n",
    "one_session_forest_recalls_1_mean = np.mean(one_session_forest_recalls_1)\n",
    "one_session_forest_recalls_1_std = np.std(one_session_forest_recalls_1)\n",
    "print('Overall recall:    ' +\n",
    "      f'{(100*one_session_forest_recalls_1_mean):.2f}'\n",
    "      + '±'\n",
    "      + f'{(100*one_session_forest_recalls_1_std):.2f}')\n",
    "print()\n",
    "\n",
    "print('--- Number of Visual Changes Recognized by Random Forest Classifier (averaged over folds) ---')\n",
    "print(str(np.sum(one_session_avg_visual_changes))) # sum over all pages\n",
    "print()\n",
    "        \n",
    "print('#################################################################')\n",
    "print()\n",
    "\n",
    "# Same-Site learner\n",
    "same_site_importances = defaultdict(list)\n",
    "same_site_forest_precisions_1 = []\n",
    "same_site_forest_recalls_1 = []\n",
    "print('Same-Site-Learning (4-fold cross valiation, three sessions as training, one session as test)')\n",
    "avg_class0_f1 = 0.0\n",
    "avg_class1_f1 = 0.0\n",
    "i = 0\n",
    "for name, sites in categories.items():\n",
    "    print()\n",
    "    print('#------------------- ' + name.center(14) + ' -------------------#')\n",
    "    print()\n",
    "    for site in sites:\n",
    "        print('                  <<<' + site.center(14) + '>>>                  ')\n",
    "        \n",
    "        # Learn\n",
    "        same_site_learner = SameSiteLearner(site)\n",
    "        \n",
    "        # Print detailed report\n",
    "        same_site_learner.print_report()\n",
    "        \n",
    "        # Collect importances\n",
    "        for feature_name, importances in same_site_learner.importances.items():\n",
    "            same_site_importances[feature_name] += importances\n",
    "            \n",
    "        # Collect precision and recall\n",
    "        same_site_forest_precisions_1.append(same_site_learner.classifier_metrics['forest']['precision_1'][0]) # zero for mean instead of std\n",
    "        same_site_forest_recalls_1.append(same_site_learner.classifier_metrics['forest']['recall_1'][0])\n",
    "            \n",
    "        # Collect f1-scores\n",
    "        avg_class0_f1 += same_site_learner.avg_class0_f1\n",
    "        avg_class1_f1 += same_site_learner.avg_class1_f1\n",
    "        i += 1\n",
    "        print()\n",
    "        \n",
    "# Normalize f1-scores and report\n",
    "avg_class0_f1 /= i\n",
    "avg_class1_f1 /= i\n",
    "print('Averaged F1-Score of class 0: ' + str(avg_class0_f1))\n",
    "print('Averaged F1-Score of class 1: ' + str(avg_class1_f1))\n",
    "print()\n",
    "\n",
    "print('--- Ranked Feature Importances (already in percentage) ---')\n",
    "print_ranked_importances(same_site_importances)\n",
    "print()\n",
    "\n",
    "# Precision and recall for visual change\n",
    "print('--- Overall Precision and Recall (already in percentage) ---')\n",
    "same_site_forest_precisions_1_mean = np.mean(same_site_forest_precisions_1)\n",
    "same_site_forest_precisions_1_std = np.std(same_site_forest_precisions_1)\n",
    "print('Overall precision: '\n",
    "      + f'{(100*same_site_forest_precisions_1_mean):.2f}'\n",
    "      + '±'\n",
    "      + f'{(100*same_site_forest_precisions_1_std):.2f}')\n",
    "same_site_forest_recalls_1_mean = np.mean(same_site_forest_recalls_1)\n",
    "same_site_forest_recalls_1_std = np.std(same_site_forest_recalls_1)\n",
    "print('Overall recall:    '\n",
    "      + f'{(100*same_site_forest_recalls_1_mean):.2f}'\n",
    "      + '±'\n",
    "      + f'{(100*same_site_forest_recalls_1_std):.2f}')\n",
    "print()\n",
    "        \n",
    "print('#################################################################')\n",
    "print()\n",
    "\n",
    "# Same-Category learner\n",
    "same_category_importances = defaultdict(list)\n",
    "print('Same-Category-Learning (4-fold cross valiation, three sites as training, one site as test)')\n",
    "for name, sites in categories.items():\n",
    "    print()\n",
    "    print('#------------------- ' + name.center(14) + ' -------------------#')\n",
    "    \n",
    "    # Learn\n",
    "    same_category_learner = SameCategoryLearner(sites)\n",
    "    \n",
    "    # Print detailed report\n",
    "    same_category_learner.print_report()\n",
    "\n",
    "    # Collect importances\n",
    "    for feature_name, importances in same_category_learner.importances.items():\n",
    "        same_category_importances[feature_name] += importances\n",
    "print()\n",
    "\n",
    "print('--- Ranked Feature Importances (already in percentage) ---')\n",
    "print_ranked_importances(same_category_importances)\n",
    "\n",
    "print('#################################################################')\n",
    "print()\n",
    "    \n",
    "# Across-Category learner\n",
    "across_category_importances = defaultdict(list)\n",
    "print('Across-Category-Learning (three categories as training, one category as test)')\n",
    "for name in categories.keys():\n",
    "    print()\n",
    "    print('#------------------- ' + name.center(14) + ' -------------------#')\n",
    "    \n",
    "    # Learn\n",
    "    across_category_learner = AcrossCategoryLearner(categories, name)\n",
    "    \n",
    "    # Print detailed report\n",
    "    across_category_learner.print_report()\n",
    "    \n",
    "    # Collect importances\n",
    "    for feature_name, importances in across_category_learner.importances.items():\n",
    "        across_category_importances[feature_name] += importances\n",
    "        \n",
    "print('--- Ranked Feature Importances (already in percentage) ---')\n",
    "print_ranked_importances(across_category_importances)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
