{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "\n",
    "The scripts computes the measures \"coverage\" and \"overflow\" to compare computed shots with the ground truth shots. It gives indication about how well the visual change classifier works.\n",
    "\n",
    "The script takes the visual change dataset and considers the root layer information, only. Each label as \"visual change\" from one frame to another is considered as shot boundary on the root layer. These shots are called \"ground truth shots\".\n",
    "\n",
    "It is also created a visual change classifier on basis of the labeling of one participant's session and computes \"visual change\" labels on all participants' sessions on one Web site. These labels are also considered for the root layer, only, and used again as shot boundaries. These shots are called \"computed shots\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Coverage\" and \"overflow\" measures are inspired from:\n",
    "\n",
    "`J. Vendrig and M. Worring, \"Systematic evaluation of logical story unit segmentation,\" in IEEE Transactions on Multimedia, vol. 4, no. 4, pp. 492-499, Dec. 2002.\n",
    "doi: 10.1109/TMM.2002.802021\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os.path\n",
    "from collections import defaultdict\n",
    "\n",
    "# Custom modules\n",
    "from classifier import Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "min_obs_extent = 32 # extent too which obs are withdrawn\n",
    "baseline_feature_name = 'pixel_diff_count_bgr'\n",
    "classifier_names = ['svc', 'forest', 'baseline']\n",
    "\n",
    "# Defines\n",
    "dataset_visual_change_dir = r'C:/StimuliDiscoveryData/Dataset_visual_change' # SET ME!\n",
    "participants = ['p1', 'p2', 'p3', 'p4']\n",
    "training_participants = ['p1']\n",
    "exclude_training_from_test = False\n",
    "\n",
    "# Categories\n",
    "shopping = ['walmart', 'amazon', 'steam']\n",
    "news = ['reddit', 'cnn', 'guardian']\n",
    "health = ['nih', 'webmd', 'mayo']\n",
    "cars = ['gm', 'nissan', 'kia']\n",
    "categories = {'shopping': shopping, 'news': news, 'health': health, 'cars': cars}\n",
    "\n",
    "# Drop and filter features\n",
    "features_drop = [\n",
    "    'bag_of_words_vocabulary_size',\n",
    "    'optical_flow_angle_min',\n",
    "    'optical_flow_angle_max',\n",
    "    'optical_flow_magnitude_min']\n",
    "features_filter = [ # not applied if empty (all features but dropped ones are then considered)\n",
    "    'edge_change_fraction',\n",
    "    'mssim_b',\n",
    "    'mssim_g',\n",
    "    'mssim_r',\n",
    "    'pixel_diff_acc_b',\n",
    "    'pixel_diff_acc_bgr',\n",
    "    'pixel_diff_acc_g',\n",
    "    'pixel_diff_acc_gray',\n",
    "    'pixel_diff_acc_hue',\n",
    "    'pixel_diff_acc_lightness',\n",
    "    'pixel_diff_acc_r',\n",
    "    'pixel_diff_acc_saturation',\n",
    "    'pixel_diff_count_b',\n",
    "    'pixel_diff_count_bgr',\n",
    "    'pixel_diff_count_g',\n",
    "    'pixel_diff_count_gray',\n",
    "    'pixel_diff_count_hue',\n",
    "    'pixel_diff_count_lightness',\n",
    "    'pixel_diff_count_r',\n",
    "    'pixel_diff_count_saturation',\n",
    "    'psnr',\n",
    "    'sift_match',\n",
    "    'sift_match_0',\n",
    "    'sift_match_16',\n",
    "    'sift_match_256',\n",
    "    'sift_match_4',\n",
    "    'sift_match_512',\n",
    "    'sift_match_64',\n",
    "    'sift_match_distance_max',\n",
    "    'sift_match_distance_mean',\n",
    "    'sift_match_distance_min',\n",
    "    'sift_match_distance_stddev',\n",
    "    'sift_match_spatial'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Session class holds one site visit by a participant\n",
    "class Session:\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self, participant, site):\n",
    "        \n",
    "        # Store some members of general interest\n",
    "        self.p = participant\n",
    "        self.s = site\n",
    "        \n",
    "        # Load dataset (header of features dataset has extra comma)\n",
    "        self.f_df = pd.read_csv(dataset_visual_change_dir + '/' + participant + '/' + site + '_features.csv')\n",
    "        self.mf_df = pd.read_csv(dataset_visual_change_dir + '/' + participant + '/' + site + '_features_meta.csv')\n",
    "        self.l1_df = pd.read_csv(dataset_visual_change_dir + '/' + participant + '/' + site + '_labels-l1.csv', header=None, names=['label'])\n",
    "        self.m_df = pd.read_csv(dataset_visual_change_dir + '/' + participant + '/' + site + '_meta.csv')\n",
    "        \n",
    "        # Drop columns of non-interest\n",
    "        self.f_df = self.f_df.drop(features_drop, axis=1)\n",
    "        \n",
    "        # Filter for columns of interest\n",
    "        if len(features_filter) > 0:\n",
    "            self.f_df = self.f_df.filter(items=features_filter, axis=1)\n",
    "        \n",
    "        # Drop observations that are smaller than a certain extent\n",
    "        width_idxs = self.mf_df[self.mf_df['overlap_width'] <= min_obs_extent].index\n",
    "        height_idxs = self.mf_df[self.mf_df['overlap_height'] <= min_obs_extent].index\n",
    "        drop_idxs = list(set(width_idxs) | set(height_idxs))\n",
    "        self.f_df = self.f_df.drop(drop_idxs, axis=0)\n",
    "        self.mf_df = self.mf_df.drop(drop_idxs, axis=0)\n",
    "        self.l1_df = self.l1_df.drop(drop_idxs, axis=0)\n",
    "        \n",
    "        # Replace some values\n",
    "        if 'optical_flow_magnitude_max' in self.f_df.columns:\n",
    "\n",
    "            # Replace infinity datapoints in 'optical_flow_magnitude_max' with maximum value (encoded as -1)\n",
    "            max_value = self.f_df['optical_flow_magnitude_max'].max() # maximum from complete training data\n",
    "\n",
    "            # In both, training an test data\n",
    "            self.f_df[self.f_df['optical_flow_magnitude_max'] == -1] = max_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute shots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_shots(df, frame_count):\n",
    "\n",
    "    # Filter for layer ('root') and label 1\n",
    "    df = df.loc[df['layer_type'] == 'root']\n",
    "    df = df.loc[df['label'] == 1]\n",
    "\n",
    "    # Shots\n",
    "    end_frames = list(df['prev_video_frame'])\n",
    "    end_frames.append(frame_count-1)\n",
    "    shots = []\n",
    "    for i in range(len(end_frames)):\n",
    "        start_frame = 0\n",
    "        prev_i = i - 1\n",
    "        if prev_i >= 0:\n",
    "            start_frame = end_frames[prev_i] + 1\n",
    "        end_frame = end_frames[i]\n",
    "        shots.append((start_frame, end_frame))\n",
    "    return shots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coverage(gt_shots, c_shots):\n",
    "\n",
    "    overall_cover = 0.0\n",
    "\n",
    "    # Go over ground truth shots\n",
    "    for (gt_start, gt_end) in gt_shots:\n",
    "\n",
    "        shot_length = gt_end - gt_start + 1\n",
    "        max_cover = 0\n",
    "        gt_range = range(gt_start, gt_end+1)\n",
    "\n",
    "        # Go over computed shots and find one with maximum coverage\n",
    "        for (c_start, c_end) in c_shots:\n",
    "            c_range = range(c_start, c_end+1)\n",
    "            cover = len(list(set(gt_range) & set(c_range)))\n",
    "            if cover > max_cover:\n",
    "                max_cover = cover\n",
    "\n",
    "        # Compute coverage\n",
    "        cover = float(max_cover) / float(shot_length)\n",
    "        overall_cover += cover * (shot_length / frame_count)\n",
    "\n",
    "    return overall_cover"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute overflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_overflow(gt_shots, c_shots):\n",
    "\n",
    "    overall_over = 0.0\n",
    "\n",
    "    # Go over ground truth shots\n",
    "    for i in range(len(gt_shots)):\n",
    "\n",
    "        (gt_start, gt_end) = gt_shots[i]\n",
    "        shot_length = gt_end - gt_start + 1\n",
    "        gt_range = range(gt_start, gt_end+1)\n",
    "\n",
    "        # Get previous ground truth shot\n",
    "        prev_gt_range = []\n",
    "        prev_gt_i = i-1\n",
    "        if prev_gt_i >= 0:\n",
    "            (prev_gt_start, prev_gt_end) = gt_shots[prev_gt_i]\n",
    "            prev_gt_range = range(prev_gt_start, prev_gt_end+1)\n",
    "\n",
    "        # Get next ground truth shot\n",
    "        next_gt_range = []\n",
    "        next_gt_i = i+1\n",
    "        if next_gt_i < len(gt_shots):\n",
    "            (next_gt_start, next_gt_end) = gt_shots[next_gt_i]\n",
    "            next_gt_range = range(next_gt_start, next_gt_end+1)\n",
    "\n",
    "        denom = float(len(prev_gt_range) + len(next_gt_range))\n",
    "        nom = 0.0\n",
    "\n",
    "        # Go over computed shots\n",
    "        for (c_start, c_end) in c_shots:\n",
    "            c_range = range(c_start, c_end+1)\n",
    "\n",
    "            # Check for intersection between frames from ground truth shot and computed shot\n",
    "            intersection_gt = len(list(set(gt_range) & set(c_range)))\n",
    "            intersection_gt_at_all = min(intersection_gt, 1) # limit to 0 or 1\n",
    "            \n",
    "            # Count only frames from computed shot that intersect (aka overflow...) with prev or next ground truth shot\n",
    "            intersection_gt_prev = len(list(set(prev_gt_range) & set(c_range)))\n",
    "            intersection_gt_next = len(list(set(next_gt_range) & set(c_range)))\n",
    "            \n",
    "            # Compute potential overflow\n",
    "            nom += max(intersection_gt_prev, intersection_gt_next) * intersection_gt_at_all\n",
    "\n",
    "        # Compute overflow\n",
    "        over = nom/denom\n",
    "        overall_over += over * (shot_length / frame_count)\n",
    "    \n",
    "    return overall_over"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sites_measures = {} # for each site, holds a dict mapping from training session to tuple of coverage and overflow\n",
    "for name, sites in categories.items():\n",
    "    for site in sites:\n",
    "        \n",
    "        print('Site: ' + site)\n",
    "        \n",
    "        measures_per_training = {} # maps from training session to tuple of coverage and overflow\n",
    "        \n",
    "        # Go over participants chosen for training\n",
    "        for training_p in training_participants:\n",
    "            \n",
    "            print('Training with: ' + training_p)\n",
    "            coverages = defaultdict(list)\n",
    "            overflows = defaultdict(list)\n",
    "\n",
    "            # Create training data\n",
    "            training_session = Session(training_p, site)\n",
    "            X_train = training_session.f_df.values\n",
    "            y_train = training_session.l1_df.values.flatten()\n",
    "            idx_baseline = training_session.f_df.columns.get_loc(baseline_feature_name)\n",
    "\n",
    "            # Go over participants that are not used for training\n",
    "            test_participants = list(participants)\n",
    "            if exclude_training_from_test:\n",
    "                test_participants.remove(training_p)\n",
    "            for p in test_participants:\n",
    "\n",
    "                # Load session to work on\n",
    "                session = Session(p, site)\n",
    "                frame_count = int(session.m_df['screencast_frame_total_count'])\n",
    "\n",
    "                # Create classifier\n",
    "                classifier = Classifier()\n",
    "\n",
    "                # Create dataframes with ground truth labels and the computed labels\n",
    "                gt_df = pd.concat([session.mf_df, session.l1_df], axis=1)\n",
    "                c_df = pd.concat([session.mf_df, session.l1_df], axis=1)\n",
    "\n",
    "                # Apply classifier to create computed labels\n",
    "                pred = classifier.apply(X_train, y_train, session.f_df.values, idx_baseline)\n",
    "\n",
    "                # Go over available classifiers and perform computation for each\n",
    "                for name in classifier_names:\n",
    "                    c_df['label'] = pred[name] # overwrite labels for computed shots\n",
    "\n",
    "                    # Compute shots\n",
    "                    gt_shots = compute_shots(gt_df, frame_count)\n",
    "                    c_shots = compute_shots(c_df, frame_count)\n",
    "\n",
    "                    # Compute measurements\n",
    "                    coverage = compute_coverage(gt_shots, c_shots)\n",
    "                    overflow = compute_overflow(gt_shots, c_shots)\n",
    "                    coverages[name].append(coverage)# dict that maps classifier name to coverage\n",
    "                    overflows[name].append(overflow)# dict that maps classifier name to overflow\n",
    "\n",
    "                    # Output information\n",
    "                    '''\n",
    "                    print(\n",
    "                        p + ':'\n",
    "                        + ' coverage = ' + '{:1.2f}'.format(coverage)\n",
    "                        + ', overflow = ' + '{:1.2f}'.format(overflow)\n",
    "                        + ', gt_shots = ' + '{:3}'.format(len(gt_shots))\n",
    "                        + ', c_shots = '  + '{:3}'.format(len(c_shots)))\n",
    "                    '''\n",
    "\n",
    "            # Store results for one session as training\n",
    "            measures_per_training[training_p] = (coverages, overflows)\n",
    "            \n",
    "            '''\n",
    "            print(\n",
    "            '$' + '{:1.2f}'.format(np.mean(coverages['svc'])) + '\\\\pm' + '{:1.2f}'.format(np.std(coverages['svc']) + '$')\n",
    "            + ', overflow = ' + '{:1.2f}'.format(np.mean(overflows['svc'])) + '\\\\pm' + '{:1.2f}'.format(np.std(overflows['svc'])))\n",
    "            '''\n",
    "        \n",
    "        # Store results across all sessions as training\n",
    "        sites_measures[site] = measures_per_training  \n",
    "\n",
    "# Print sites\n",
    "for site, _ in sites_measures.items():\n",
    "    print(site, end=' ')\n",
    "print()\n",
    "\n",
    "print('Coverage')\n",
    "    \n",
    "# Print coverages\n",
    "for _, measures_per_training in sites_measures.items(): # go over sites\n",
    "    svc_means = []\n",
    "    forest_means = []\n",
    "    baseline_means = []\n",
    "    for training_p in training_participants:\n",
    "        svc_means.append(np.mean(measures_per_training[training_p][0]['svc']))\n",
    "        forest_means.append(np.mean(measures_per_training[training_p][0]['forest']))\n",
    "        baseline_means.append(np.mean(measures_per_training[training_p][0]['baseline']))\n",
    "        \n",
    "    print(\n",
    "        '$' + '{:1.2f}'.format(np.mean(svc_means)) + '\\\\pm' + '{:1.2f}'.format(np.std(svc_means)) + '$'\n",
    "        + ' & '\n",
    "        + '$' + '{:1.2f}'.format(np.mean(forest_means)) + '\\\\pm' + '{:1.2f}'.format(np.std(forest_means)) + '$'\n",
    "        + ' & '\n",
    "        + '$' + '{:1.2f}'.format(np.mean(baseline_means)) + '\\\\pm' + '{:1.2f}'.format(np.std(baseline_means)) + '$'\n",
    "        + ' & ')\n",
    "print()\n",
    "\n",
    "print('Overflow')\n",
    "\n",
    "# Print overflows\n",
    "for _, measures_per_training in sites_measures.items(): # go over sites\n",
    "    svc_means = []\n",
    "    forest_means = []\n",
    "    baseline_means = []\n",
    "    for training_p in training_participants:\n",
    "        svc_means.append(np.mean(measures_per_training[training_p][1]['svc']))\n",
    "        forest_means.append(np.mean(measures_per_training[training_p][1]['forest']))\n",
    "        baseline_means.append(np.mean(measures_per_training[training_p][1]['baseline']))\n",
    "        \n",
    "    print(\n",
    "        '$' + '{:1.2f}'.format(np.mean(svc_means)) + '\\\\pm' + '{:1.2f}'.format(np.std(svc_means)) + '$'\n",
    "        + ' & '\n",
    "        + '$' + '{:1.2f}'.format(np.mean(forest_means)) + '\\\\pm' + '{:1.2f}'.format(np.std(forest_means)) + '$'\n",
    "        + ' & '\n",
    "        + '$' + '{:1.2f}'.format(np.mean(baseline_means)) + '\\\\pm' + '{:1.2f}'.format(np.std(baseline_means)) + '$'\n",
    "        + ' & ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
